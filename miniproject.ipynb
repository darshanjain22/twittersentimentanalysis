{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.8.10 64-bit"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.8.10","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":["<h2 style=\"text-align:center;\">PUNE INSTITUE OF COMPUTER TECHNOLOGY<h2>\n","<h3 style=\"text-align:center;\">MINI PROJECT<h3>\n","<h2 style=\"text-align:center;\">TWITTER SENTIMENT ANALYSIS</h2></p>"],"metadata":{}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-danger\">  \n","<h2><center><strong>Importing Python Libraries üìï üìó üìò üìô</strong></center></h2>\n","        \n","</div>"],"metadata":{"papermill":{"duration":0.065466,"end_time":"2020-11-30T07:38:51.578836","exception":false,"start_time":"2020-11-30T07:38:51.51337","status":"completed"},"tags":[]}},{"cell_type":"code","execution_count":2,"source":["import numpy as np \n","import pandas as pd \n","import seaborn as sns \n","import matplotlib.pyplot as plt\n","plt.style.use('ggplot')\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.metrics import classification_report, confusion_matrix\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer \n","from sklearn.model_selection import train_test_split\n","from mlxtend.plotting import plot_confusion_matrix\n","import matplotlib.cm as cm\n","from matplotlib import rcParams\n","from collections import Counter\n","from nltk.tokenize import RegexpTokenizer\n","import re\n","import string\n","from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing import sequence\n","%matplotlib inline\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'numpy'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_11273/2817183247.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ggplot'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"]}],"metadata":{"id":"-E4TvIVLMa73","scrolled":true,"trusted":true}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-danger\">  \n","<h2><center><strong>Loading the data üìÅ üìÇ</strong></center></h2>\n","        \n","</div>"],"metadata":{"papermill":{"duration":0.065466,"end_time":"2020-11-30T07:38:51.578836","exception":false,"start_time":"2020-11-30T07:38:51.51337","status":"completed"},"tags":[]}},{"cell_type":"code","execution_count":null,"source":["data = pd.read_csv(\"https://www.kaggle.com/kazanova/sentiment140\", encoding = \"ISO-8859-1\", engine=\"python\")\n","data.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-danger\">  \n","<h2><center><strong>Exploratory data analysis üîé üìä</strong></center></h2>\n","        \n","</div>"],"metadata":{"papermill":{"duration":0.065466,"end_time":"2020-11-30T07:38:51.578836","exception":false,"start_time":"2020-11-30T07:38:51.51337","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":["#### Five top records of data"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["data.head()"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["#### Five last records of data"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["data.tail()"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["#### Coloumns/features in data"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["data.columns"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["#### Length of data"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["print('lenght of data is', len(data))"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["#### Shape of data"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["data.shape"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["#### Data information"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["data.info()"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["#### Data types of all coloumns"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["data.dtypes"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["#### Checking Null values"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["np.sum(data.isnull().any(axis=1))"],"outputs":[],"metadata":{"scrolled":true,"trusted":true}},{"cell_type":"markdown","source":["#### Rows and columns in the dataset"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["print('Count of columns in the data is:  ', len(data.columns))"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["print('Count of rows in the data is:  ', len(data))"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["<h1><center><strong>Data Preparation üìù</strong></center></h1>"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["data=data[['text','label']]"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["data['label'][data['label']==4]=1"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["data_pos = data[data['label'] == 1]\n","data_neg = data[data['label'] == 0]"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["data_pos = data_pos.iloc[:int(20000)]\n","data_neg = data_neg.iloc[:int(20000)]"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["data = pd.concat([data_pos, data_neg])"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["data['text']=data['text'].str.lower()"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["data['text'].tail()"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["#### Cleaning and removing Stop words of english"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["stopwords_list = stopwords.words('english')"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["from nltk.corpus import stopwords\n","\", \".join(stopwords.words('english'))"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["#### Cleaning and removing the above stop words list from the tweet text"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["STOPWORDS = set(stopwords.words('english'))\n","def cleaning_stopwords(text):\n","    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n","data['text'] = data['text'].apply(lambda text: cleaning_stopwords(text))\n","data['text'].head()"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["#### Cleaning and removing punctuations "],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["english_punctuations = string.punctuation\n","punctuations_list = english_punctuations\n","def cleaning_punctuations(text):\n","    translator = str.maketrans('', '', punctuations_list)\n","    return text.translate(translator)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["data['text']= data['text'].apply(lambda x: cleaning_punctuations(x))\n","data['text'].tail()"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["#### Cleaning and removing repeating characters"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["def cleaning_repeating_char(text):\n","    return re.sub(r'(.)\\1+', r'\\1', text)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["data['text'] = data['text'].apply(lambda x: cleaning_repeating_char(x))\n","data['text'].tail()"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["#### Cleaning and removing email"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["def cleaning_email(data):\n","    return re.sub('@[^\\s]+', ' ', data)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["data['text']= data['text'].apply(lambda x: cleaning_email(x))\n","data['text'].tail()"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["#### Cleaning and removing URL's"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["def cleaning_URLs(data):\n","    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',data)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["data['text'] = data['text'].apply(lambda x: cleaning_URLs(x))\n","data['text'].tail()"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["#### Cleaning and removing Numeric numbers"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["def cleaning_numbers(data):\n","    return re.sub('[0-9]+', '', data)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["data['text'] = data['text'].apply(lambda x: cleaning_numbers(x))\n","data['text'].tail()"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["#### Getting tokenization of tweet text"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["tokenizer = RegexpTokenizer(r'\\w+')\n","data['text'] = data['text'].apply(tokenizer.tokenize)"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["data['text'].head()"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["#### Applying Stemming"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["st = nltk.PorterStemmer()\n","def stemming_on_text(data):\n","    text = [st.stem(word) for word in data]\n","    return data\n","\n","data['text']= data['text'].apply(lambda x: stemming_on_text(x))"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["data['text'].head()"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["#### Applying Lemmatizer"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["lm = nltk.WordNetLemmatizer()\n","def lemmatizer_on_text(data):\n","    text = [lm.lemmatize(word) for word in data]\n","    return data\n","\n","data['text'] = data['text'].apply(lambda x: lemmatizer_on_text(x))"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["data['text'].head()"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["####  Separating input feature and label"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["X=data.text\n","y=data.label"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["max_len = 500\n","tok = Tokenizer(num_words=2000)\n","tok.fit_on_texts(X)\n","sequences = tok.texts_to_sequences(X)\n","sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"],"outputs":[],"metadata":{"id":"plIFObh4jYJV","outputId":"cc21833e-e053-454b-d410-b8e9d48b4392","trusted":true}},{"cell_type":"code","execution_count":null,"source":["sequences_matrix.shape"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["#### Separating the 70% data for training data and 30% for testing data"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["X_train, X_test, Y_train, Y_test = train_test_split(sequences_matrix, y, test_size=0.3, random_state=2)"],"outputs":[],"metadata":{"id":"oOlkTTg4oRqR","trusted":true}},{"cell_type":"markdown","source":["<div class=\"alert alert-block alert-danger\">  \n","<h2><center><strong>Implementing Tensorflow based model for training üß™</strong></center></h2>   \n","</div>"],"metadata":{"papermill":{"duration":0.065466,"end_time":"2020-11-30T07:38:51.578836","exception":false,"start_time":"2020-11-30T07:38:51.51337","status":"completed"},"tags":[]}},{"cell_type":"code","execution_count":null,"source":["def tensorflow_based_model(): #Defined tensorflow_based_model function for training tenforflow based model\n","    inputs = Input(name='inputs',shape=[max_len])#step1\n","    layer = Embedding(2000,50,input_length=max_len)(inputs) #step2\n","    layer = LSTM(64)(layer) #step3\n","    layer = Dense(256,name='FC1')(layer) #step4\n","    layer = Activation('relu')(layer) # step5\n","    layer = Dropout(0.5)(layer) # step6\n","    layer = Dense(1,name='out_layer')(layer) #step4 again but this time its giving only one output as because we need to classify the tweet as positive or negative\n","    layer = Activation('sigmoid')(layer) #step5 but this time activation function is sigmoid for only one output.\n","    model = Model(inputs=inputs,outputs=layer) #here we are getting the final output value in the model for classification\n","    return model #function returning the value when we call it"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"markdown","source":["# Model compilation"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["model = tensorflow_based_model() # here we are calling the function of created model\n","model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])  "],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["history=model.fit(X_train,Y_train,batch_size=80,epochs=6, validation_split=0.1)# here we are starting the training of model by feeding the training data\n","print('Training finished !!')"],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["accr1 = model.evaluate(X_test,Y_test) #we are starting to test the model here"],"outputs":[],"metadata":{"executionInfo":{"elapsed":2136,"status":"ok","timestamp":1590595289545,"user":{"displayName":"Muhammad Imran Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYO6GnhoI_aryKI-bhtIReU4wH4wLPGZdwWVtS=s64","userId":"06817026978292405035"},"user_tz":-300},"id":"LlxD3pU9U0ws","outputId":"a1adf83f-f07c-4f10-fc1c-a895f6e39d56","trusted":true}},{"cell_type":"code","execution_count":null,"source":["print('Test set\\n  Accuracy: {:0.2f}'.format(accr1[1])) #the accuracy of the model on test data is given below"],"outputs":[],"metadata":{"executionInfo":{"elapsed":3304,"status":"ok","timestamp":1590596501745,"user":{"displayName":"Muhammad Imran Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYO6GnhoI_aryKI-bhtIReU4wH4wLPGZdwWVtS=s64","userId":"06817026978292405035"},"user_tz":-300},"id":"E2bQq4jaU0wt","outputId":"fe61d1b5-954a-4770-d423-ab897f64b495","trusted":true}},{"cell_type":"code","execution_count":null,"source":["y_pred = model.predict(X_test) #getting predictions on the trained model\n","y_pred = (y_pred > 0.5) "],"outputs":[],"metadata":{"trusted":true}},{"cell_type":"code","execution_count":null,"source":["print('\\n')\n","print(\"confusion matrix\")\n","print('\\n')\n","CR=confusion_matrix(Y_test, y_pred)\n","print(CR)\n","print('\\n')\n","\n","fig, ax = plot_confusion_matrix(conf_mat=CR,figsize=(10, 10),\n","                                show_absolute=True,\n","                                show_normed=True,\n","                                colorbar=True)\n","plt.show()"],"outputs":[],"metadata":{"executionInfo":{"elapsed":17851,"status":"ok","timestamp":1590596595973,"user":{"displayName":"Muhammad Imran Zaman","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYO6GnhoI_aryKI-bhtIReU4wH4wLPGZdwWVtS=s64","userId":"06817026978292405035"},"user_tz":-300},"id":"WERD7KXs8YmQ","outputId":"db07021b-ec87-4be8-ce17-14cffe8749a6","trusted":true}},{"cell_type":"markdown","source":[" \n","<h1><center><strong>Conclusion üìù</strong></center></h1>\n","    <p>\n","<li>We used the twitter sentiment analysis dataset and explored the data with different ways.</li>\n","        <li>We prepared the text data of tweets by removing the unnecessary things.</li>\n","          <li>We trained model based on tensorflow with all settings. </li>\n","        <li>We evaluated the model with different evaluation measures.</li>\n","        </p>"],"metadata":{}}]}